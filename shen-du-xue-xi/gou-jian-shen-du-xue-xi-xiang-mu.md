## 计划

我们将开发分为四个阶段，最后三个阶段在多次迭代中进行。

- 项目研究
- 模型设计
- 实现及调试
- 实验及调参

需要把控的是：

- 快速实现第一版策略，后面的迭代才是真正花时间的地方。
- 多使用开源的、现成的东西，不要重复造轮子

## 数据集合

好的数据集合的特征:

- 类别均衡
- 数据充足
- 数据和标记中有高质量信息
- 数据和标记错误非常小
- 与你的问题相关

需要注意的问题：

- 关于类别平衡，实际上优先和问题同分布，不过不平衡相当影响神经网络的学习。
- 实验的时候不一定使用全量数据，耗时太长
- 收集足够的样本。如果样本不够，应考虑迁移学习。
- 有意识的迭代地样本，比如分析错误并过滤掉与实际问题无关的样本；

    数据没有好坏之分，只是有些数据不能满足你的需求。此外，随着样本类别的增加，训练和保持输出质量会变得更加困难，删除不相关的数据可以得到一个更好的模型。


## 设计细节

### 框架选择及成果复用

- 现在 tensorflow 还是主流。（2018-04）

- 尽量对前人的成果进行复用。比如使用别人 pre-train  embedding 


###  Cost function  

成本函数的选择

并非所有的成本函数都是等价的，它会影响模型的训练难度。有些问题合适 成本函数的标准（默认），有些需要根据问题进行修改。

一些是一些简单原则：

- 分类问题：交叉熵，折页损失函数（SVM）
- 回归： 均方误差（MSE）
- 对象检测或分割：交并比（IoU）
- 策略优化：KL 散度
- 词嵌入：噪音对比估计（NCE）
- 词向量：余弦相似度

注意:

- 在理论分析中看起来不错的成本函数在实践中可能不太好用。
- 成本函数可以是部分猜测加部分实验，也可以是几个成本函数的组合。

### Metrics 

度量标准

良好的度量标准有助于更好地比较和调整模型。

**技巧**：对于特殊问题，请查看 Kaggle 平台，该平台组织了许多 DL 竞赛，并提供了详细的度量标准。 

### Regularization 

正则化

> L1 正则化和 L2 正则化都很常见，但 L2 正则化在深度学习中更受欢迎。

L1 正则化可以产生更加稀疏的参数，使网络更易于解释，因此适合做特征选择，但是L1 对异常值很敏感，解可能不稳定。所以解更稳定的L2使用的更多。


### Gradient descent 

梯度下降

这是一个需要监控的指标，主要关注视梯度是否消失或爆炸。

首先排除bug：

- 检测到小梯度,很大可能是程序bug, 如输入数据未正确缩放或权重全部初始化为零。

解决方法：

- apply gradient clipping (in particular for NLP) when gradient explode.


### Scaling 

特征归一

通常将特征缩放为以零为均值在特定范围内，如 [-1, 1]。特征的不适当缩放是梯度爆炸或降低的一个最常见的原因。

常用方法：从训练数据中计算均值和方差，以使数据更接近正态分布。

注意的地方是：如果缩放验证或测试数据，要再次利用训练数据的均值和方差。



### Batch Normalization & Layer Normalization 


批归一化 缩写 BN  ,层归一化 缩写 LN

每层激活函数之前节点输出的不平衡性是引起梯度问题的另一个主要原因。

BN 可以提高了准确度，同时缩短了训练时间。

注意：

- 在CNN 中使用 使用BN 一般很有效果
- BN 对 RNN 无效，我们需要使用LN。



### Dropout

2015 Batch Normalization 兴起后，大家现在使用 dropout 越来越少了

### Activation functions 激活函数

### Split dataset 拆分数据集

### baseline

### Checkpoints 检查点

### Randomization


## 参考

[如何从零开始构建深度学习项目？这里有一份详细的教程](https://mp.weixin.qq.com/s/qpqqeSaRwQyBJlo3P25q6g )